{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true,
   "authorship_tag": "ABX9TyOUbrDBd3VNqT+PJkXAqqPY"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial no. 1 of SpeechBrain-MOABB: Setting up EEG decoding"
   ],
   "metadata": {
    "id": "gxG3UuKZrsW6"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnRSS3jGxVsk"
   },
   "source": [
    "## **Prerequisites**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download SpeechBrain-MOABB\n",
    "\n",
    "SpeechBrain-MOABB can be downloaded from the GitHub repository listed below."
   ],
   "metadata": {
    "id": "-GhrvkOFT9_t"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KSOTkqPsJXxZ"
   },
   "source": [
    "!git clone https://github.com/speechbrain/benchmarks"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxr8nZ7bbP15"
   },
   "source": [
    "### Install SpeechBrain and SpeechBrain-MOABB requirements, and install SpeechBrain"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VFEX28mu4dFt"
   },
   "source": [
    "%%capture\n",
    "!pip install speechbrain\n",
    "\n",
    "%cd /content/benchmarks/benchmarks/MOABB\n",
    "!pip install -r extra-requirements.txt # Install additional dependencies"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Define a yaml file containing the hyper-parameters defining a decoding pipeline**\n",
    "\n",
    "Let us address 4-class motor imagery decoding using BNCI2014-001 dataset (also known as \"BCI IV2a dataset\"), by adopting a leave-one-session-out strategy using the first participants' signals, leaving out the session named '0test'. EEGNet is used as decoder.\n",
    "\n",
    "You can set all hyper-parameters to specific values if you already know them; otherwise, you can set them to placeholders (i.e., as `!PLACEHOLDER`). For example, folders (e.g., the data folder, the folder for compressed dataset, and the output folder), and dataset information (e.g., data iterator, index of subject and session to use) are usually kept as placeholders.\n",
    "\n",
    "Before start writing the yaml file, please follow the SpeechBrain tutorial dedicated to HyperPyYAML at https://speechbrain.github.io/tutorial_basics.html."
   ],
   "metadata": {
    "id": "f45uzf3mUNyc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from hyperpyyaml import load_hyperpyyaml, dump_hyperpyyaml\n",
    "\n",
    "example_hyperparams = \"\"\"\n",
    "seed: 1234\n",
    "__set_torchseed: !apply:torch.manual_seed [!ref <seed>]\n",
    "\n",
    "# DIRECTORIES\n",
    "data_folder: !PLACEHOLDER  #'/path/to/dataset'. The dataset will be automatically downloaded in this folder\n",
    "cached_data_folder: !PLACEHOLDER #'path/to/pickled/dataset'\n",
    "output_folder: !PLACEHOLDER #'path/to/results'\n",
    "\n",
    "# DATASET HPARS\n",
    "# Defining the MOABB dataset.\n",
    "dataset: !new:moabb.datasets.BNCI2014001\n",
    "save_prepared_dataset: True # set to True if you want to save the prepared dataset as a pkl file to load and use afterwards\n",
    "data_iterator_name: 'leave-one-session-out'\n",
    "target_subject_idx: 0\n",
    "target_session_idx: 1\n",
    "events_to_load: null # all events will be loaded\n",
    "original_sample_rate: 250 # Original sampling rate provided by dataset authors\n",
    "sample_rate: 125 # Target sampling rate (Hz)\n",
    "# band-pass filtering cut-off frequencies\n",
    "fmin: 1\n",
    "fmax: 40\n",
    "n_classes: 4\n",
    "tmin: 0.\n",
    "tmax: 4.0\n",
    "# number of steps used when selecting adjacent channels from a seed channel (default at Cz)\n",
    "n_steps_channel_selection: 3\n",
    "T: !apply:math.ceil\n",
    "    - !ref <sample_rate> * (<tmax> - <tmin>)\n",
    "C: 22\n",
    "test_with: 'best' # 'last' or 'best'\n",
    "test_key: \"acc\" # Possible opts: \"loss\", \"f1\", \"auc\", \"acc\"\n",
    "\n",
    "# METRICS\n",
    "f1: !name:sklearn.metrics.f1_score\n",
    "    average: 'macro'\n",
    "acc: !name:sklearn.metrics.balanced_accuracy_score\n",
    "cm: !name:sklearn.metrics.confusion_matrix\n",
    "metrics:\n",
    "    f1: !ref <f1>\n",
    "    acc: !ref <acc>\n",
    "    cm: !ref <cm>\n",
    "\n",
    "# TRAINING HPARS\n",
    "n_train_examples: 100  # it will be replaced in the train script\n",
    "# checkpoints to average\n",
    "avg_models: 10\n",
    "number_of_epochs: 1000\n",
    "lr: 0.0001\n",
    "# Learning rate scheduling (cyclic learning rate is used here)\n",
    "max_lr: !ref <lr> # Upper bound of the cycle (max value of the lr)\n",
    "base_lr: 0.00000001 # Lower bound in the cycle (min value of the lr)\n",
    "step_size_multiplier: 5 #from 2 to 8\n",
    "step_size: !apply:round\n",
    "    - !ref <step_size_multiplier> * <n_train_examples> / <batch_size>\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n",
    "    base_lr: !ref <base_lr>\n",
    "    max_lr: !ref <max_lr>\n",
    "    step_size: !ref <step_size>\n",
    "label_smoothing: 0.0\n",
    "loss: !name:speechbrain.nnet.losses.nll_loss\n",
    "    label_smoothing: !ref <label_smoothing>\n",
    "optimizer: !name:torch.optim.Adam\n",
    "    lr: !ref <lr>\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter  # epoch counter\n",
    "    limit: !ref <number_of_epochs>\n",
    "batch_size: 32\n",
    "valid_ratio: 0.2\n",
    "\n",
    "# DATA NORMALIZATION\n",
    "dims_to_normalize: 1 # 1 (time) or 2 (EEG channels)\n",
    "normalize: !name:speechbrain.processing.signal_processing.mean_std_norm\n",
    "    dims: !ref <dims_to_normalize>\n",
    "\n",
    "# MODEL\n",
    "input_shape: [null, !ref <T>, !ref <C>, null]\n",
    "cnn_temporal_kernels: 8\n",
    "cnn_temporal_kernelsize: 62\n",
    "cnn_spatial_depth_multiplier: 2\n",
    "cnn_spatial_max_norm: 1.\n",
    "cnn_spatial_pool: 4\n",
    "cnn_septemporal_depth_multiplier: 1\n",
    "cnn_septemporal_point_kernels: !ref <cnn_temporal_kernels> * <cnn_spatial_depth_multiplier> * <cnn_septemporal_depth_multiplier>\n",
    "cnn_septemporal_kernelsize: 16\n",
    "cnn_septemporal_pool: 8\n",
    "cnn_pool_type: 'avg'\n",
    "dense_max_norm: 0.25\n",
    "dropout: 0.5\n",
    "activation_type: 'elu'\n",
    "\n",
    "model: !new:models.EEGNet.EEGNet\n",
    "    input_shape: !ref <input_shape>\n",
    "    cnn_temporal_kernels: !ref <cnn_temporal_kernels>\n",
    "    cnn_temporal_kernelsize: [!ref <cnn_temporal_kernelsize>, 1]\n",
    "    cnn_spatial_depth_multiplier: !ref <cnn_spatial_depth_multiplier>\n",
    "    cnn_spatial_max_norm: !ref <cnn_spatial_max_norm>\n",
    "    cnn_spatial_pool: [!ref <cnn_spatial_pool>, 1]\n",
    "    cnn_septemporal_depth_multiplier: !ref <cnn_septemporal_depth_multiplier>\n",
    "    cnn_septemporal_point_kernels: !ref <cnn_septemporal_point_kernels>\n",
    "    cnn_septemporal_kernelsize: [!ref <cnn_septemporal_kernelsize>, 1]\n",
    "    cnn_septemporal_pool: [!ref <cnn_septemporal_pool>, 1]\n",
    "    cnn_pool_type: !ref <cnn_pool_type>\n",
    "    activation_type: !ref <activation_type>\n",
    "    dense_max_norm: !ref <dense_max_norm>\n",
    "    dropout: !ref <dropout>\n",
    "    dense_n_neurons: !ref <n_classes>\n",
    "\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "id": "f_m7D3eiUbHC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the yaml file on disk\n",
    "f = open('/content/example_hyperparams.yaml', \"w\")\n",
    "f.write(example_hyperparams)\n",
    "f.close()"
   ],
   "metadata": {
    "id": "DbJcCq8fYgL-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Note about data augmentation**\n",
    "\n",
    "It is worth highlighting that in the previous yaml file, no data augmentation was included. However, you can easily add data augmentation by defining each augmenter (e.g., applying CutCat and random time shift).\n",
    "\n",
    "The so-defined augmenters are provided as input to the `Augmenter` class, that will combine and apply the augmenters. For instance, you can perform the augmenters in sequence or in parallel (`parallel_augment` input parameter), use one or more augmenters for augmenting each mini-batch of data (`min_augmentations` and `max_augmentations` input parameters), and repeat data augmentation multiple times for each mini-batch (`repeat_augment` input parameter). See `Augmenter` documentation for further details."
   ],
   "metadata": {
    "id": "mGsMSB2RvZX5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_augmentation_hyperparams = \"\"\"\n",
    "# DATA AUGMENTATION\n",
    "# cutcat (disabled when min_num_segments=max_num_segments=1)\n",
    "max_num_segments: 3\n",
    "cutcat: !new:speechbrain.processing.speech_augmentation.CutCat\n",
    "    min_num_segments: 2\n",
    "    max_num_segments: !ref <max_num_segments>\n",
    "\n",
    "# random shifts between -250 ms to 250 ms (disabled when shift_delta=0.)\n",
    "shift_delta: 0.25\n",
    "min_shift: !apply:math.floor\n",
    "    - !ref 0 - <sample_rate> * <shift_delta>\n",
    "max_shift: !apply:math.floor\n",
    "    - !ref 0 + <sample_rate> * <shift_delta>\n",
    "time_shift: !new:speechbrain.processing.speech_augmentation.RandomShift\n",
    "    min_shift: !ref <min_shift>\n",
    "    max_shift: !ref <max_shift>\n",
    "    dim: 1\n",
    "\n",
    "repeat_augment: 1\n",
    "augment: !new:speechbrain.processing.augmentation.Augmenter\n",
    "    parallel_augment: True\n",
    "    concat_original: True\n",
    "    parallel_augment_fixed_bs: True\n",
    "    repeat_augment: !ref <repeat_augment>\n",
    "    shuffle_augmentations: True\n",
    "    min_augmentations: 1\n",
    "    max_augmentations: 2\n",
    "    cutcat: !ref <cutcat>\n",
    "    time_shift: !ref <time_shift>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "example_hyperparams += data_augmentation_hyperparams"
   ],
   "metadata": {
    "id": "PtMD8LoevYg4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Train the neural network on a single cross-validation fold**\n",
    "\n",
    "Start network training by running the `train.py` script providing the filepath to the yaml file and by overriding the variables set as placeholders in the yaml file. Furthermore, here for brevity we override also the number of training epochs to 50 epochs (instead of 1000 epochs) with `--number_of_epochs 50`. It is worth highlighting that you can also override any other hyper-parameters in the same way."
   ],
   "metadata": {
    "id": "qiOA1AlVg48b"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python train.py /content/example_hyperparams.yaml \\\n",
    "--data_folder '/content/data/BNCI2014001' \\\n",
    "--cached_data_folder '/content/data' \\\n",
    "--output_folder '/content/results/single-fold-example/BNCI2014001' \\\n",
    "--data_iterator_name 'leave-one-session-out' \\\n",
    "--target_subject_idx 0 \\\n",
    "--target_session_idx 1 \\\n",
    "--number_of_epochs 50\n"
   ],
   "metadata": {
    "id": "pYEa8oubbvk-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Run a complete experiment by looping over the entire dataset**\n",
    "\n",
    "In the previous cell, `train.py` was called for a single cross-validation fold (e.g., one participant and one held-out session in leave-one-session-out cross-validation). Thus, we provide a command line interface for easily running training on all participants and cross-validation folds (using `./run_experiments.sh`).\n",
    "\n",
    "Here, besides the relevant folders, you should specify the hyper-parameter file, the number of participants and sessions to use, the data iteration scheme (leave-one-session-out or leave-one-subject-out). In addition, you can also run the code multiple times, each time with a different random seed used for initializing weights (by setting the `nruns` parameters). Finally, you can define the `device` to use (set to `cpu` if you do not have a GPU).\n",
    "\n"
   ],
   "metadata": {
    "id": "ITKUTG0JUcha"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!./run_experiments.sh --hparams /content/example_hyperparams.yaml \\\n",
    "--data_folder '/content/data/BNCI2014001'\\\n",
    "--cached_data_folder '/content/data' \\\n",
    "--output_folder '/content/results/full-experiment/BNCI2014001' \\\n",
    "--nsbj 9 --nsess 2 --nruns 1 --train_mode 'leave-one-session-out' \\\n",
    "--number_of_epochs 50 \\\n",
    "--device=cuda"
   ],
   "metadata": {
    "id": "mOD1JbH2TSZj"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}