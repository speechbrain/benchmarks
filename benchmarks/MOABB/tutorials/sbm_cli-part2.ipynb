{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": [
    "JnRSS3jGxVsk",
    "-GhrvkOFT9_t"
   ],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyNWLtCvcoEy4w785FdYahnS"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial no. 2 of SpeechBrain-MOABB: Setting up hyper-parameter tuning"
   ],
   "metadata": {
    "id": "FLmjnvhyullP"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnRSS3jGxVsk"
   },
   "source": [
    "## **Prerequisites**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download SpeechBrain-MOABB\n",
    "\n",
    "SpeechBrain-MOABB can be downloaded from the GitHub repository listed below."
   ],
   "metadata": {
    "id": "-GhrvkOFT9_t"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KSOTkqPsJXxZ"
   },
   "source": [
    "!git clone https://github.com/speechbrain/benchmarks"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxr8nZ7bbP15"
   },
   "source": [
    "### Install SpeechBrain and SpeechBrain-MOABB requirements, and install SpeechBrain"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VFEX28mu4dFt"
   },
   "source": [
    "%%capture\n",
    "!pip install speechbrain\n",
    "\n",
    "%cd /content/benchmarks/benchmarks/MOABB\n",
    "!pip install -r extra-requirements.txt # Install additional dependencies\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Define the yaml file including the hyper-parameter search space**\n",
    "\n",
    "Let us use the same yaml file as in the *Tutorial no. 1 of SpeechBrain-MOABB: Setting up EEG decoding*. However, in this case we assume that some hyper-parameters are not optimal. For example, we assume that the low and high cut-off frequencies for band-pass filtering should be optimized, together with the number of epochs, the learning rate, and few network hyper-parameters (e.g., the number of convolutional kernels and the kernel size of the first layer of EEGNet).\n",
    "\n",
    "We provide a CLI for performing hyper-parameter search, by using the `./run_hparam_optimization.sh` script, that performs the hyper-parameter search iterations calling multiple times the `./run_experiments.sh` script. The script assumes that Orion flags are directly included in the specified YAML hyper-parameter file using comments. Thus, you can easily define the search space for each hyper-parameter by commenting:\n",
    "\n",
    "```yaml\n",
    "dropout: 0.1748  # @orion_step1: --dropout~\"uniform(0.0, 0.5)\"\n",
    "```\n",
    "\n",
    "In this case, dropout rate will be sampled using an uniform distribution between 0 and 0.5. See Orion documentation for the supported distributions.\n",
    "\n",
    "`./run_hparam_optimization.sh` supports multi-step hyper-parameter optimization.\n",
    "Briefly, you can optimize a subset of hyper-parameters while keeping the others fixed. After finding their optimal values, we utilize them as a foundation for optimizing another set of hyper-parameters. Furthermore, once hyper-parameter tuning is completed, the optimal decoding pipeline is re-trained and re-evaluated for N times for providing a robust evaluation of the performance, to reduce the variability of the performance due to different random initializations (seed variability), by setting the parameter `nruns_eval`. Besides the options provided by the `./run_experiments.sh` CLI, `./run_hparam_optimization.sh` introduces other options. For example, the user can change the amount of signals to use during hyper-parameter search, crucial for reducing computational time on large datasets, by setting the number of participants and sessions to use for hyper-parameter search (`nsbj_hpsearch`, `nsess_hpsearch`).\n",
    "\n",
    "To optimize a hyper-parameter in a second step, follow this syntax in the YAML file:\n",
    "\n",
    "```yaml\n",
    "# cutcat (disabled when min_num_segments=max_num_segments=1)\n",
    "max_num_segments: 6 # @orion_step2: --max_num_segments~\"uniform(2, 6, discrete=True)\"\n",
    "```\n",
    "\n",
    "For brevity, the number of epochs was tuned here only between 50 and 200 epochs."
   ],
   "metadata": {
    "id": "f45uzf3mUNyc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tuned_hyperparams = \"\"\"\n",
    "# DATASET HPARS\n",
    "fmin: 1 # @orion_step1: --fmin~\"uniform(0.1, 5, precision=2)\"\n",
    "fmax: 40 # @orion_step1: --fmax~\"uniform(20.0, 50.0, precision=3)\"\n",
    "\n",
    "# TRAINING HPARS\n",
    "number_of_epochs: 1000 # @orion_step1: --number_of_epochs~\"uniform(50, 200, discrete=True)\"\n",
    "lr: 0.0001 # @orion_step1: --lr~\"choices([0.01, 0.005, 0.001, 0.0005, 0.0001])\"\n",
    "\n",
    "# MODEL\n",
    "cnn_temporal_kernels: 8 # @orion_step1: --cnn_temporal_kernels~\"uniform(4, 64,discrete=True)\"\n",
    "cnn_temporal_kernelsize: 62 # @orion_step1: --cnn_temporal_kernelsize~\"uniform(24, 62,discrete=True)\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "other_hyperparams = \"\"\"\n",
    "seed: 1234\n",
    "__set_torchseed: !apply:torch.manual_seed [!ref <seed>]\n",
    "\n",
    "# DIRECTORIES\n",
    "data_folder: !PLACEHOLDER  #'/path/to/dataset'. The dataset will be automatically downloaded in this folder\n",
    "cached_data_folder: !PLACEHOLDER #'path/to/pickled/dataset'\n",
    "output_folder: !PLACEHOLDER #'path/to/results'\n",
    "\n",
    "# DATASET HPARS\n",
    "# Defining the MOABB dataset.\n",
    "dataset: !new:moabb.datasets.BNCI2014001\n",
    "save_prepared_dataset: True # set to True if you want to save the prepared dataset as a pkl file to load and use afterwards\n",
    "data_iterator_name: 'leave-one-session-out'\n",
    "target_subject_idx: 0\n",
    "target_session_idx: 1\n",
    "events_to_load: null # all events will be loaded\n",
    "original_sample_rate: 250 # Original sampling rate provided by dataset authors\n",
    "sample_rate: 125 # Target sampling rate (Hz)\n",
    "# band-pass filtering cut-off frequencies\n",
    "n_classes: 4\n",
    "tmin: 0.\n",
    "tmax: 4.0\n",
    "# number of steps used when selecting adjacent channels from a seed channel (default at Cz)\n",
    "n_steps_channel_selection: 3\n",
    "T: !apply:math.ceil\n",
    "    - !ref <sample_rate> * (<tmax> - <tmin>)\n",
    "C: 22\n",
    "test_with: 'last' # 'last' or 'best'\n",
    "test_key: \"acc\" # Possible opts: \"loss\", \"f1\", \"auc\", \"acc\"\n",
    "\n",
    "# METRICS\n",
    "f1: !name:sklearn.metrics.f1_score\n",
    "    average: 'macro'\n",
    "acc: !name:sklearn.metrics.balanced_accuracy_score\n",
    "cm: !name:sklearn.metrics.confusion_matrix\n",
    "metrics:\n",
    "    f1: !ref <f1>\n",
    "    acc: !ref <acc>\n",
    "    cm: !ref <cm>\n",
    "\n",
    "# TRAINING HPARS\n",
    "n_train_examples: 100  # it will be replaced in the train script\n",
    "# checkpoints to average\n",
    "avg_models: 10\n",
    "# Learning rate scheduling (cyclic learning rate is used here)\n",
    "max_lr: !ref <lr> # Upper bound of the cycle (max value of the lr)\n",
    "base_lr: 0.00000001 # Lower bound in the cycle (min value of the lr)\n",
    "step_size_multiplier: 5 #from 2 to 8\n",
    "step_size: !apply:round\n",
    "    - !ref <step_size_multiplier> * <n_train_examples> / <batch_size>\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n",
    "    base_lr: !ref <base_lr>\n",
    "    max_lr: !ref <max_lr>\n",
    "    step_size: !ref <step_size>\n",
    "label_smoothing: 0.0\n",
    "loss: !name:speechbrain.nnet.losses.nll_loss\n",
    "    label_smoothing: !ref <label_smoothing>\n",
    "optimizer: !name:torch.optim.Adam\n",
    "    lr: !ref <lr>\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter  # epoch counter\n",
    "    limit: !ref <number_of_epochs>\n",
    "batch_size: 32\n",
    "valid_ratio: 0.2\n",
    "\n",
    "# DATA NORMALIZATION\n",
    "dims_to_normalize: 1 # 1 (time) or 2 (EEG channels)\n",
    "normalize: !name:speechbrain.processing.signal_processing.mean_std_norm\n",
    "    dims: !ref <dims_to_normalize>\n",
    "\n",
    "# MODEL\n",
    "input_shape: [null, !ref <T>, !ref <C>, null]\n",
    "cnn_spatial_depth_multiplier: 2\n",
    "cnn_spatial_max_norm: 1.\n",
    "cnn_spatial_pool: 4\n",
    "cnn_septemporal_depth_multiplier: 1\n",
    "cnn_septemporal_point_kernels: !ref <cnn_temporal_kernels> * <cnn_spatial_depth_multiplier> * <cnn_septemporal_depth_multiplier>\n",
    "cnn_septemporal_kernelsize: 16\n",
    "cnn_septemporal_pool: 8\n",
    "cnn_pool_type: 'avg'\n",
    "dense_max_norm: 0.25\n",
    "dropout: 0.5\n",
    "activation_type: 'elu'\n",
    "\n",
    "model: !new:models.EEGNet.EEGNet\n",
    "    input_shape: !ref <input_shape>\n",
    "    cnn_temporal_kernels: !ref <cnn_temporal_kernels>\n",
    "    cnn_temporal_kernelsize: [!ref <cnn_temporal_kernelsize>, 1]\n",
    "    cnn_spatial_depth_multiplier: !ref <cnn_spatial_depth_multiplier>\n",
    "    cnn_spatial_max_norm: !ref <cnn_spatial_max_norm>\n",
    "    cnn_spatial_pool: [!ref <cnn_spatial_pool>, 1]\n",
    "    cnn_septemporal_depth_multiplier: !ref <cnn_septemporal_depth_multiplier>\n",
    "    cnn_septemporal_point_kernels: !ref <cnn_septemporal_point_kernels>\n",
    "    cnn_septemporal_kernelsize: [!ref <cnn_septemporal_kernelsize>, 1]\n",
    "    cnn_septemporal_pool: [!ref <cnn_septemporal_pool>, 1]\n",
    "    cnn_pool_type: !ref <cnn_pool_type>\n",
    "    activation_type: !ref <activation_type>\n",
    "    dense_max_norm: !ref <dense_max_norm>\n",
    "    dropout: !ref <dropout>\n",
    "    dense_n_neurons: !ref <n_classes>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sample_hyperparams = tuned_hyperparams + other_hyperparams"
   ],
   "metadata": {
    "id": "f_m7D3eiUbHC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the yaml file on disk\n",
    "f = open('/content/sample_hyperparams.yaml', \"w\")\n",
    "f.write(sample_hyperparams)\n",
    "f.close()"
   ],
   "metadata": {
    "id": "DbJcCq8fYgL-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!./run_hparam_optimization.sh --hparams '/content/sample_hyperparams.yaml' \\\n",
    "--data_folder '/content/data/BNCI2014001'\\\n",
    "--cached_data_folder '/content/data' \\\n",
    "--output_folder '/content/results/hyperparameter-search/BNCI2014001' \\\n",
    "--nsbj 9 --nsess 2 --nruns 1 --train_mode 'leave-one-session-out' \\\n",
    "--exp_name 'hyperparameter-search' \\\n",
    "--nsbj_hpsearch 1 --nsess_hpsearch 1 \\\n",
    "--nruns_eval 1 \\\n",
    "--eval_metric acc \\\n",
    "--exp_max_trials 50"
   ],
   "metadata": {
    "id": "VJaBar0Iuw3H"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}